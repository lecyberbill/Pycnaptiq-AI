{
  "name": "Entraînement LoRA (Interne)",
  "tab_name": "Entraînement LoRA",
  "init_function": "initialize",
  "language": {
    "fr": {
      "florence2_task_analyze": "Analyser (Florence-2)",
      "florence2_task_generate_tags": "Générer des Tags (Florence-2)",
      "florence2_task_mixed_caption": "Légende Mixte (Florence-2)",
      "florence2_task_mixed_caption_plus": "Légende Mixte Plus (Florence-2)",
      "img2txt_stop_button_label": "Arrêt de l'entraînement LoRA en cours...",
      "lora_training_tab_title": "Entraînement de LoRA",
      "lora_training_module_title": "Assistant d'Entraînement LoRA (Interne)",
      "lora_input_images_dir_label": "Dossier des images d'entraînement",
      "lora_input_images_dir_info": "Sélectionnez le dossier contenant vos images.",
      "lora_trigger_word_label": "Mot-clé (Trigger Word)",
      "lora_trigger_word_placeholder": "ex: mylora, personnageXYZ",
      "lora_concept_label": "Concept (pour nom de dossier de données)",
      "lora_concept_placeholder": "ex: style_entrainement, personnage_data",
      "lora_output_name_label": "Nom du LoRA de sortie (sans .safetensors)",
      "lora_output_name_placeholder": "ex: mon_super_lora",
      "lora_base_model_label": "Modèle SDXL de base pour l'entraînement",
      "lora_base_model_info": "Chemin vers le fichier .safetensors du modèle SDXL.",
      "lora_base_model_placeholder": "/chemin/vers/votre/modele_sdxl.safetensors",
      "lora_training_project_dir_label": "Dossier du projet d'entraînement LoRA",
      "lora_training_project_dir_info": "Dossier où les données préparées et le LoRA seront sauvegardés.",
      "lora_training_project_dir_placeholder": "/chemin/vers/votre/projet_lora",
      "lora_caption_task_label": "Tâche pour la génération de légendes (Florence-2)",
      "lora_caption_task_info": "Choisissez la précision des légendes générées.",
      "lora_data_preparation_title": "Préparation des Données",
      "lora_training_parameters_title": "Paramètres d'Entraînement",
      "lora_advanced_network_options_title": "Options Réseau LoRA",
      "lora_optimizer_options_title": "Options de l'Optimiseur",
      "lora_epochs_label": "Nombre d'Epochs",
      "lora_learning_rate_label": "Taux d'apprentissage (Learning Rate)",
      "lora_learning_rate_placeholder": "ex: 1e-4, 5e-5",
      "lora_batch_size_label": "Taille du Batch (Batch Size)",
      "lora_resolution_label": "Résolution d'entraînement",
      "lora_network_dim_label": "Dimension du réseau (Rank / Dim)",
      "lora_network_alpha_label": "Alpha du réseau",
      "lora_train_unet_only_label": "Entraîner uniquement l'UNet",
      "lora_train_text_encoder_label": "Entraîner aussi les Text Encoders (plus de VRAM)",
      "lora_optimizer_label": "Optimiseur",
      "lora_lr_scheduler_label": "Planificateur de Taux d'Apprentissage (LR Scheduler)",
      "lora_mixed_precision_label": "Précision Mixte (Mixed Precision)",
      "lora_save_every_n_epochs_label": "Sauvegarder tous les N epochs (0 pour désactiver)",
      "lora_training_status_label": "Statut de l'entraînement",
      "lora_log_output_label": "Logs de l'entraînement",
      "lora_preparing_data": "Préparation des données en cours...",
      "lora_data_prepared_in": "Données préparées dans : {path}",
      "lora_starting_training": "Démarrage de l'entraînement LoRA...",
      "lora_training_in_progress": "Entraînement en cours...",
      "lora_training_epoch_progress": "Epoch {current_epoch}/{total_epochs}, Step {current_step}/{total_steps}, Loss: {loss:.4f}",
      "lora_training_failed": "L'entraînement LoRA a échoué.",
      "lora_training_stopped_by_user": "Entraînement arrêté par l'utilisateur.",
      "lora_error_no_images_dir": "Veuillez sélectionner un dossier d'images.",
      "lora_error_images_dir_not_found": "Le dossier d'images spécifié n'existe pas : {path}",
      "lora_error_no_images_in_dir": "Aucune image trouvée dans le dossier : {path}",
      "lora_error_no_trigger_word": "Veuillez entrer un mot-clé (trigger word).",
      "lora_error_no_concept": "Veuillez entrer un concept pour le nom du dossier de données.",
      "lora_error_no_output_name": "Veuillez entrer un nom pour le LoRA de sortie.",
      "lora_error_no_base_model": "Veuillez spécifier le chemin du modèle SDXL de base.",
      "lora_error_base_model_not_found": "Modèle SDXL de base non trouvé : {path}",
      "lora_error_no_project_dir": "Veuillez spécifier un dossier de projet pour l'entraînement.",
      "lora_captioning_images": "Génération des légendes...",
      "lora_caption_model_unloaded": "Modèle de génération de légendes déchargé.",
      "lora_error_data_preparation": "Erreur lors de la préparation des données",
      "lora_error_training_process": "Erreur lors du processus d'entraînement",
      "lora_training_already_running": "Un entraînement est déjà en cours.",
      "lora_stopping_training": "Arrêt de l'entraînement demandé...",
      "lora_model_loading_for_train": "Chargement du modèle de base pour l'entraînement...",
      "lora_dataset_creation_failed": "Échec de la création du dataset d'entraînement.",
      "lora_peft_config_error": "Erreur lors de la configuration PEFT/LoRA.",
      "lora_optimizer_setup_error": "Erreur lors de la configuration de l'optimiseur/scheduler.",
      "lora_saving_checkpoint": "Sauvegarde du checkpoint LoRA epoch {epoch}...",
      "lora_checkpoint_saved": "Checkpoint LoRA sauvegardé : {path}",
      "lora_error_invalid_lr": "Erreur: Taux d'apprentissage invalide: {lr}",
      "lora_debug_path_construction": "DEBUG Construction Chemin: models_dir={models_dir}, selected_name={selected_name}, constructed_path={constructed_path}",
      "lora_error_base_model_path_not_exist": "Erreur: Le chemin du modèle de base n'existe pas: {path}",
      "lora_error_base_model_not_a_file": "Erreur: Le chemin du modèle de base n'est pas un fichier: {path}",
      "lora_base_model_path_validated": "INFO: Chemin du modèle de base validé: {path}",
      "lora_loading_pipeline_from_single_file": "Chargement du pipeline depuis le fichier unique : {path}",
      "lora_pipeline_loaded_successfully": "Pipeline SDXL chargé avec succès.",
      "lora_extracting_components_from_pipeline": "Extraction des composants depuis le pipeline...",
      "lora_components_extracted_and_configured": "Composants extraits et configurés pour l'entraînement.",
      "lora_full_pipeline_object_deleted": "Objet pipeline complet supprimé pour libérer la mémoire.",
      "lora_configuring_peft_model": "Configuration du modèle PEFT (LoRA) pour l'UNet...",
      "lora_peft_model_configured": "Modèle PEFT (LoRA) configuré.",
      "lora_train_te_not_fully_implemented": "AVERTISSEMENT: L'entraînement des Text Encoders avec LoRA n'est pas complètement implémenté.",
      "lora_error_loading_base_model_components": "Erreur lors du chargement des composants du modèle de base depuis {path}: {error}",
      "lora_bnb_not_found_adamw": "AVERTISSEMENT: bitsandbytes non trouvé. AdamW8bit non disponible, utilisation de AdamW standard.",
      "lora_lion_not_found_adamw": "AVERTISSEMENT: lion-pytorch non trouvé. Lion non disponible, utilisation de AdamW standard.",
      "lora_training_ready": "Prêt pour l'entraînement LoRA.",
      "lora_lr_5e_4_desc": "Très rapide (5e-4), pour tests, risque élevé d'instabilité.",
      "lora_lr_1e_4_desc": "Rapide (1e-4), bon point de départ pour de nombreux concepts.",
      "lora_lr_5e_5_desc": "Modéré (5e-5), souvent plus stable pour les styles ou détails fins.",
      "lora_lr_2e_5_desc": "Lent (2e-5), pour un affinage précis, nécessite plus d'epochs.",
      "lora_lr_1e_5_desc": "Très lent (1e-5), pour un affinage très fin, peut nécessiter beaucoup d'epochs.",
      "lora_learning_rate_info_dropdown": "Sélectionnez un taux d'apprentissage. Des valeurs plus faibles sont plus stables mais plus lentes.",
      "florence2_task_caption": "Légende simple",
      "florence2_task_detailed_caption": "Légende détaillée",
      "florence2_task_more_detailed_caption": "Légende très détaillée",
      "florence2_task_ocr": "OCR (Texte dans l'image)",
      "florence2_task_ocr_with_region": "OCR avec régions (Texte et boîtes)",
      "lora_epoch_end_summary": "Fin Epoch {epoch}: Perte moyenne = {avg_loss:.4f}",
      "lora_trainable_parameters_unet": "Paramètres entraînables (UNet): {trainable} / {total} ({percentage:.2f}%)",
      "lora_trainable_parameters_te1": "Paramètres entraînables (Text Encoder 1): {trainable} / {total} ({percentage:.2f}%)",
      "lora_trainable_parameters_te2": "Paramètres entraînables (Text Encoder 2): {trainable} / {total} ({percentage:.2f}%)",
      "lora_text_encoders_lora_added": "Adaptateurs LoRA ajoutés aux Text Encoders.",
      "lora_checkpoint_save_error": "Erreur lors de la sauvegarde du checkpoint epoch {epoch}: {error}",
      "lora_debug_shapes": "DEBUG Shapes - prompt_embeds: {prompt_embeds_shape}, pooled_prompt_embeds: {pooled_prompt_embeds_shape}, add_time_ids: {add_time_ids_shape}, unet.add_embedding.linear_1.weight: {unet_add_embedding_linear1_weight_shape}",
      "lora_debug_shapes_error": "Erreur lors de la récupération des formes de débogage : {error}",
      "lora_debug_unet_input_shapes": "DEBUG UNet Inputs - prompt_embeds: {prompt_embeds_shape}, pooled_prompt_embeds: {pooled_prompt_embeds_shape}, add_time_ids: {add_time_ids_shape}, unet.add_embedding.linear_1.weight: {unet_add_embedding_linear1_weight_shape}",      
      "lora_total_training_time": "Temps total d'entraînement : {time} secondes.",
      "lora_prepare_data_button": "Préparer les Données",
      "lora_start_training_button": "Démarrer l'Entraînement",
      "lora_process_already_running": "Un processus de préparation ou d'entraînement est déjà en cours.",
      "lora_stopping_preparation": "Arrêt de la préparation des données demandé...",
      "lora_error_missing_input_prep": "Erreur: Des informations sont manquantes pour la préparation des données (nom LoRA, dossier projet, etc.).",
      "lora_verifying_prepared_data": "Vérification des données préparées...",
      "lora_error_prepared_data_not_found": "Erreur: Le dossier de données préparées '{path}' est introuvable ou vide. Veuillez d'abord préparer les données.",
      "lora_prepared_data_verified": "Données préparées vérifiées dans '{path}'.",
      "lora_data_preparation_complete": "Préparation des données terminée. Vous pouvez maintenant vérifier les légendes et démarrer l'entraînement.",
      "lora_preparation_stopped_by_user": "Préparation des données arrêtée par l'utilisateur.",
      "lora_data_preparation_failed_unexpectedly": "Erreur inattendue lors de la préparation des données: {error}",
      "lora_starting_training_phase": "Phase d'entraînement démarrée...",
      "lora_error_missing_training_params": "Erreur: Des paramètres d'entraînement essentiels sont manquants (ex: nom du LoRA, modèle de base, dossier projet).",
      "lora_auto_captioning_label": "Activer le légendage automatique (Florence-2)",
      "lora_auto_captioning_info": "Si coché, génère automatiquement les légendes. Sinon, copie les .txt existants ou utilise le mot-clé.",
      "lora_caption_saved_auto": "Légende auto. sauvegardée pour {image}: {txt_file}",
      "lora_caption_copied_manual": "Légende manuelle copiée de {source_txt} vers {dest_txt}",
      "lora_caption_skipped_manual_no_source_txt": "Légende manuelle ignorée pour {image} (pas de .txt source), utilisation du mot-clé '{trigger}'.",
      "lora_error_no_compatible_images_in_dir": "Erreur: Aucune image compatible ({extensions}) trouvée dans le dossier source: {path}",
      "lora_image_too_small_skipped": "Image {image} ({original_size}) trop petite pour la résolution cible {min_size}x{min_size}, ignorée.",
      "lora_cropping_image": "Recadrage de l'image {image} de {original_size} vers {target_size}x{target_size}.",
      "lora_training_complete_single_file": "Entraînement LoRA terminé avec succès en {time} secondes. Fichier LoRA sauvegardé : {filepath}",
      "lora_error_caption_generation": "Erreur lors de la génération de la légende pour {image}: {error}",
      "lora_final_save_error": "Erreur lors de la sauvegarde finale du LoRA: {error} vers {path}",
      "lora_renaming_files_sequentially": "Renommage séquentiel des fichiers dans le dossier de données...",
      "lora_file_renamed": "Fichier renommé: '{old}' -> '{new}'",
      "lora_renaming_complete": "Renommage séquentiel terminé. {count} fichiers images (et leurs .txt associés) ont été renommés."
    },
    "en": {
      "florence2_task_analyze": "Analyze (Florence-2)",
      "florence2_task_generate_tags": "Generate Tags (Florence-2)",
      "florence2_task_mixed_caption": "Mixed Caption (Florence-2)",
      "florence2_task_mixed_caption_plus": "Mixed Caption Plus (Florence-2)",
      "img2txt_stop_button_label": "Stopping LoRA training...",
      "lora_training_tab_title": "LoRA Training",
      "lora_training_module_title": "LoRA Training Assistant (Internal)",
      "lora_input_images_dir_label": "Training Images Directory",
      "lora_input_images_dir_info": "Select the folder containing your images.",
      "lora_trigger_word_label": "Trigger Word",
      "lora_trigger_word_placeholder": "e.g., mylora, characterXYZ",
      "lora_concept_label": "Concept (for data folder name)",
      "lora_concept_placeholder": "e.g., training_style, character_data",
      "lora_output_name_label": "Output LoRA Name (without .safetensors)",
      "lora_output_name_placeholder": "e.g., my_awesome_lora",
      "lora_base_model_label": "Base SDXL Model for Training",
      "lora_base_model_info": "Path to the .safetensors file of the SDXL model.",
      "lora_base_model_placeholder": "/path/to/your/sdxl_model.safetensors",
      "lora_training_project_dir_label": "LoRA Training Project Directory",
      "lora_training_project_dir_info": "Directory where prepared data and the LoRA will be saved.",
      "lora_training_project_dir_placeholder": "/path/to/your/lora_project",
      "lora_caption_task_label": "Task for Caption Generation (Florence-2)",
      "lora_caption_task_info": "Choose the detail level of generated captions.",
      "lora_data_preparation_title": "Data Preparation",
      "lora_training_parameters_title": "Training Parameters",
      "lora_advanced_network_options_title": "LoRA Network Options",
      "lora_optimizer_options_title": "Optimizer Options",
      "lora_epochs_label": "Number of Epochs",
      "lora_learning_rate_label": "Learning Rate",
      "lora_learning_rate_placeholder": "e.g., 1e-4, 5e-5",
      "lora_batch_size_label": "Batch Size",
      "lora_resolution_label": "Training Resolution",
      "lora_network_dim_label": "Network Dimension (Rank / Dim)",
      "lora_network_alpha_label": "Network Alpha",
      "lora_train_unet_only_label": "Train UNet only",
      "lora_train_text_encoder_label": "Train Text Encoders too (more VRAM)",
      "lora_optimizer_label": "Optimizer",
      "lora_lr_scheduler_label": "Learning Rate Scheduler",
      "lora_mixed_precision_label": "Mixed Precision",
      "lora_save_every_n_epochs_label": "Save every N epochs (0 to disable)",
      "lora_training_status_label": "Training Status",
      "lora_log_output_label": "Training Logs",
      "lora_preparing_data": "Preparing data...",
      "lora_data_prepared_in": "Data prepared in: {path}",
      "lora_starting_training": "Starting LoRA training...",
      "lora_training_in_progress": "Training in progress...",
      "lora_training_epoch_progress": "Epoch {current_epoch}/{total_epochs}, Step {current_step}/{total_steps}, Loss: {loss:.4f}",
      "lora_training_failed": "LoRA training failed.",
      "lora_training_stopped_by_user": "Training stopped by user.",
      "lora_error_no_images_dir": "Please select an image directory.",
      "lora_error_images_dir_not_found": "Specified image directory not found: {path}",
      "lora_error_no_images_in_dir": "No images found in directory: {path}",
      "lora_error_no_trigger_word": "Please enter a trigger word.",
      "lora_error_no_concept": "Please enter a concept for the data folder name.",
      "lora_error_no_output_name": "Please enter a name for the output LoRA.",
      "lora_error_no_base_model": "Please specify the path to the base SDXL model.",
      "lora_error_base_model_not_found": "Base SDXL model not found: {path}",
      "lora_error_no_project_dir": "Please specify a project directory for training.",
      "lora_captioning_images": "Generating captions...",
      "lora_caption_model_unloaded": "Caption generation model unloaded.",
      "lora_error_data_preparation": "Error during data preparation",
      "lora_error_training_process": "Error during training process",
      "lora_training_already_running": "A training process is already running.",
      "lora_stopping_training": "Requesting training stop...",
      "lora_model_loading_for_train": "Loading base model for training...",
      "lora_dataset_creation_failed": "Failed to create training dataset.",
      "lora_peft_config_error": "Error configuring PEFT/LoRA.",
      "lora_optimizer_setup_error": "Error setting up optimizer/scheduler.",
      "lora_saving_checkpoint": "Saving LoRA checkpoint epoch {epoch}...",
      "lora_checkpoint_saved": "LoRA checkpoint saved: {path}",
      "lora_error_invalid_lr": "Error: Invalid learning rate: {lr}",
      "lora_debug_path_construction": "DEBUG Path Construction: models_dir={models_dir}, selected_name={selected_name}, constructed_path={constructed_path}",
      "lora_error_base_model_path_not_exist": "Error: Base model path does not exist: {path}",
      "lora_error_base_model_not_a_file": "Error: Base model path is not a file: {path}",
      "lora_base_model_path_validated": "INFO: Base model path validated: {path}",
      "lora_loading_pipeline_from_single_file": "Loading pipeline from single file: {path}",
      "lora_pipeline_loaded_successfully": "SDXL pipeline loaded successfully.",
      "lora_extracting_components_from_pipeline": "Extracting components from pipeline...",
      "lora_components_extracted_and_configured": "Components extracted and configured for training.",
      "lora_full_pipeline_object_deleted": "Full pipeline object deleted to free memory.",
      "lora_configuring_peft_model": "Configuring PEFT (LoRA) model for UNet...",
      "lora_peft_model_configured": "PEFT (LoRA) model configured.",
      "lora_train_te_not_fully_implemented": "WARNING: Training Text Encoders with LoRA is not fully implemented.",
      "lora_error_loading_base_model_components": "Error loading base model components from {path}: {error}",
      "lora_bnb_not_found_adamw": "WARNING: bitsandbytes not found. AdamW8bit optimizer unavailable, falling back to standard AdamW.",
      "lora_lion_not_found_adamw": "WARNING: lion-pytorch not found. Lion optimizer unavailable, falling back to standard AdamW.",
      "lora_training_ready": "Ready for LoRA training.",
      "lora_lr_5e_4_desc": "Very fast (5e-4), for tests, high risk of instability.",
      "lora_lr_1e_4_desc": "Fast (1e-4), good starting point for many concepts.",
      "lora_lr_5e_5_desc": "Moderate (5e-5), often more stable for styles or fine details.",
      "lora_lr_2e_5_desc": "Slow (2e-5), for precise tuning, requires more epochs.",
      "lora_lr_1e_5_desc": "Very slow (1e-5), for very fine tuning, may require many epochs.",
      "lora_learning_rate_info_dropdown": "Select a learning rate. Lower values are more stable but slower.",
      "florence2_task_caption": "Simple Caption",
      "florence2_task_detailed_caption": "Detailed Caption",
      "florence2_task_more_detailed_caption": "Very Detailed Caption",
      "florence2_task_ocr": "OCR (Text in image)",
      "florence2_task_ocr_with_region": "OCR with regions (Text and boxes)",
      "lora_epoch_end_summary": "End Epoch {epoch}: Average Loss = {avg_loss:.4f}",
      "lora_trainable_parameters_unet": "Trainable parameters (UNet): {trainable} / {total} ({percentage:.2f}%)",
      "lora_trainable_parameters_te1": "Trainable parameters (Text Encoder 1): {trainable} / {total} ({percentage:.2f}%)",
      "lora_trainable_parameters_te2": "Trainable parameters (Text Encoder 2): {trainable} / {total} ({percentage:.2f}%)",
      "lora_text_encoders_lora_added": "LoRA adapters added to Text Encoders.",
      "lora_checkpoint_save_error": "Error saving checkpoint epoch {epoch}: {error}",
      "lora_debug_shapes": "DEBUG Shapes - prompt_embeds: {prompt_embeds_shape}, pooled_prompt_embeds: {pooled_prompt_embeds_shape}, add_time_ids: {add_time_ids_shape}, unet.add_embedding.linear_1.weight: {unet_add_embedding_linear1_weight_shape}",
      "lora_debug_shapes_error": "Error retrieving debug shapes: {error}",
      "lora_debug_unet_input_shapes": "DEBUG UNet Inputs - prompt_embeds: {prompt_embeds_shape}, pooled_prompt_embeds: {pooled_prompt_embeds_shape}, add_time_ids: {add_time_ids_shape}, unet.add_embedding.linear_1.weight: {unet_add_embedding_linear1_weight_shape}",      
      "lora_total_training_time": "Total training time: {time} seconds.",
      "lora_prepare_data_button": "Prepare Data",
      "lora_start_training_button": "Start Training",
      "lora_process_already_running": "A preparation or training process is already running.",
      "lora_stopping_preparation": "Stopping data preparation...",
      "lora_error_missing_input_prep": "Error: Missing information for data preparation (LoRA name, project dir, etc.).",
      "lora_verifying_prepared_data": "Verifying prepared data...",
      "lora_error_prepared_data_not_found": "Error: Prepared data folder '{path}' not found or empty. Please prepare data first.",
      "lora_prepared_data_verified": "Prepared data verified in '{path}'.",
      "lora_data_preparation_complete": "Data preparation complete. You can now check the captions and start training.",
      "lora_preparation_stopped_by_user": "Data preparation stopped by user.",
      "lora_data_preparation_failed_unexpectedly": "Unexpected error during data preparation: {error}",
      "lora_starting_training_phase": "Training phase started...",
      "lora_error_missing_training_params": "Error: Essential training parameters are missing (e.g., LoRA name, base model, project directory).",
      "lora_auto_captioning_label": "Enable Automatic Captioning (Florence-2)",
      "lora_auto_captioning_info": "If checked, automatically generates captions. Otherwise, copies existing .txt files or uses the trigger word.",
      "lora_caption_saved_auto": "Auto caption saved for {image}: {txt_file}",
      "lora_caption_copied_manual": "Manual caption copied from {source_txt} to {dest_txt}",
      "lora_caption_skipped_manual_no_source_txt": "Manual caption skipped for {image} (no source .txt), using trigger word '{trigger}'.",
      "lora_error_no_compatible_images_in_dir": "Error: No compatible images ({extensions}) found in source directory: {path}",
      "lora_image_too_small_skipped": "Image {image} ({original_size}) too small for target resolution {min_size}x{min_size}, skipped.",
      "lora_cropping_image": "Cropping image {image} from {original_size} to {target_size}x{target_size}.",
      "lora_training_complete_single_file": "LoRA training completed successfully in {time} seconds. LoRA file saved to: {filepath}",
      "lora_error_caption_generation": "Error generating caption for {image}: {error}",
      "lora_final_save_error": "Error saving final LoRA: {error} to {path}",
      "lora_renaming_files_sequentially": "Sequentially renaming files in the data folder...",
      "lora_file_renamed": "File renamed: '{old}' -> '{new}'",
      "lora_renaming_complete": "Sequential renaming complete. {count} image files (and their associated .txt) were renamed."
    }
  },
  "dependencies": []
}
