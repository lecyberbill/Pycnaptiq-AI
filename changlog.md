# Changelog

## üêì Beta 2.0.0 The fearless young rooster üêì - 2025-06-13

### ‚ú® New Features and Improvements ‚ú®

*   **ImageToText (`ImageToText_mod.py`)**:
    *   Improved log management: the log list and the maximum number of entries are now managed at the instance level for better encapsulation.
    *   Added a "Unload Model" button to free up Florence-2 model resources.
    *   Generation of a detailed JSON report at the end of processing, including the status of each image, the method used, processing time, etc.
    *   Direct use of `FLORENCE2_TASKS` and module translations for task mapping, enhancing robustness.

*   **LoRA Training (`LoRATraining_mod.py`)**:
    *   **Major UI and Logic Overhaul**:
        *   Clear separation of data preparation and training steps with dedicated buttons ("Prepare Data", "Start Training").
        *   Internal logic divided into `_actual_preparation_logic` and `_actual_training_logic` for better organization.
        *   Use of `queue.Queue` for non-blocking log communication from background threads to the user interface.
    *   **Enhanced Data Preparation**:
        *   Optional automatic *captioning* toggle via a checkbox.
        *   If automatic *captioning* is disabled, existing `.txt` files in the source folder are copied.
        *   Sequential renaming of images and associated `.txt` files in the prepared data folder (e.g., `concept_0001.png`, `concept_0001.txt`).
    *   **Dataset Management (`DreamBoothDataset`)**:
        *   Now stores and returns the original image sizes (`original_size_hw`) and crop coordinates (`crop_coords_top_left_yx`).
    *   **Training Logic (SDXL)**:
        *   Calculation and passing of `add_time_ids` (including original size and crop coordinates) to the UNet model, as required by SDXL.
        *   VAE encoding is performed outside the `autocast` context if the VAE is in fp32 and training is in fp16/bf16.
        *   Added *gradient clipping* during training for better convergence stability.
        *   Final saving of LoRA as a single `.safetensors` file containing UNet weights and optionally text encoders.
        *   Use of `unet.add_adapter()` and `text_encoder.add_adapter()` for modern PEFT configuration.
        *   Use of `cast_training_params()` to convert LoRA parameters to fp32 when training in fp16.
    *   **User Interface (UI)**:
        *   Learning rate selection is now a dropdown menu with descriptions for each value.
        *   Base model selection is a dropdown menu.
        *   Optimizer, learning rate scheduler, and mixed precision options are dropdown menus.
        *   Advanced network and optimizer settings are grouped into accordion sections.

*   **Memory Management (`Utils/gest_mem.py`)**:
    *   New utility module for monitoring system resource usage (RAM, CPU, VRAM, GPU Usage).
    *   Uses `psutil` for RAM and CPU statistics.
    *   Uses `pynvml` (if available) for detailed VRAM statistics and GPU usage for NVIDIA cards.
    *   Falls back to `torch.cuda` for basic VRAM information if `pynvml` is unavailable.
    *   Displays statistics via circular progress bars in the UI.
    *   Provides a "Memory Management" accordion in the UI with:
        *   Optional live display of statistics.
        *   A "Unload All Models" button interacting with `ModelManager`.
        *   Explicit memory cleanup (`gc.collect()`, `torch.cuda.empty_cache()`) after unloading.

*   **CogView3-Plus (`CogView3Plus_mod.py`)**:
    *   Improved logic for determining allowed resolutions, falling back to `FORMATS` if `COGVIEW3PLUS_ALLOWED_RESOLUTIONS` is undefined.
    *   Model loading now indicates that configurations (offload, slicing, tiling) are handled by `ModelManager`.
    *   Uses `execute_pipeline_task_async` for image generation, enabling a more responsive UI.
    *   Explicit memory cleanup (`del`, `gc.collect()`, `torch.cuda.empty_cache()`) after generating each batch.
    *   Saved image metadata includes `Module: "CogView3-Plus"` and `Model: THUDM/CogView3-Plus-3B`.

*   **CogView4 (`CogView4_mod.py`)**:
    *   Similar resolution determination logic as CogView3Plus, using `COGVIEW4_ALLOWED_RESOLUTIONS`.
    *   Model loading applies specific configurations (CPU offload, VAE slicing/tiling) *after* loading the `CogView4Pipeline`.
    *   Uses `execute_pipeline_task_async` for generation.
    *   Saved image metadata includes `Module: "CogView4"` and `Model: THUDM/CogView4-6B`.

### üêõ Bug Fixes üêõ

*   **LoRA Training (`LoRATraining_mod.py`)**:
    *   Fixed a potential bug where `is_preparing` was not properly used, replaced with `self.is_preparing`.
    *   Ensures `is_preparing` is set to `False` when training starts.

### ‚öôÔ∏è Technical and Refactoring ‚öôÔ∏è

*   **General**:
    *   Modules now use `self.module_translations`, initialized with merged translations (global + module-specific) during module initialization via `GestionModule`.

*   **Model Manager (`Utils/model_manager.py`)**:
    *   The `load_model` method now handles `sana_sprint`, `cogview4`, `cogview3plus` model types and applies specific configurations (dtype, offload) for these models.
    *   Improved `unload_model` method for explicit pipeline component removal.
    *   The `apply_loras` method has been revised to use `unload_lora_weights` and `set_adapters` more robustly.

*   **LLM Prompter (`Utils/llm_prompter_util.py`)**:
    *   Uses `AutoModelForCausalLM` and `AutoTokenizer` for broader Hugging Face model compatibility.
    *   Model is loaded on CPU (`device_map="cpu"`) to avoid VRAM conflicts.
    *   Tokenizer `pad_token` is set to `eos_token` if absent, necessary for models like Qwen.
    *   Improved parsing of LLM output to extract the prompt, handling `<think>` tags and common preambles.


## üêì b√©ta 2.0.0 The fearless young rooster üêì - 2025-06-13

### ‚ú® Nouvelles Fonctionnalit√©s et Am√©liorations ‚ú®

*   **ImageToText (`ImageToText_mod.py`)**:
    *   Am√©lioration de la gestion des logs : la liste des logs et le nombre maximum d'entr√©es sont d√©sormais g√©r√©s au niveau de l'instance pour une meilleure encapsulation.
    *   Ajout d'un bouton "D√©charger le mod√®le" permettant de lib√©rer les ressources du mod√®le Florence-2.
    *   G√©n√©ration d'un rapport JSON d√©taill√© √† la fin du traitement, incluant le statut de chaque image, la m√©thode utilis√©e, le temps de traitement, etc.
    *   Utilisation directe de `FLORENCE2_TASKS` et des traductions du module pour le mappage des t√¢ches, am√©liorant la robustesse.

*   **Entra√Ænement LoRA (`LoRATraining_mod.py`)**:
    *   **Refonte Majeure de l'Interface et de la Logique**:
        *   S√©paration claire des √©tapes de pr√©paration des donn√©es et d'entra√Ænement avec des boutons d√©di√©s ("Pr√©parer les Donn√©es", "D√©marrer l'Entra√Ænement").
        *   La logique interne a √©t√© divis√©e en `_actual_preparation_logic` et `_actual_training_logic` pour une meilleure organisation.
        *   Utilisation de `queue.Queue` pour une communication non bloquante des logs depuis les threads d'arri√®re-plan vers l'interface utilisateur.
    *   **Pr√©paration des Donn√©es Am√©lior√©e**:
        *   Option de *captioning* automatique d√©sactivable via une case √† cocher.
        *   Si le *captioning* automatique est d√©sactiv√©, les fichiers `.txt` existants dans le dossier source sont copi√©s.
        *   Renommage s√©quentiel des images et des fichiers `.txt` associ√©s dans le dossier de donn√©es pr√©par√© (ex: `concept_0001.png`, `concept_0001.txt`).
    *   **Gestion du Dataset (`DreamBoothDataset`)**:
        *   Stocke et retourne maintenant les tailles originales des images (`original_size_hw`) et les coordonn√©es de recadrage (`crop_coords_top_left_yx`).
    *   **Logique d'Entra√Ænement (SDXL)**:
        *   Calcul et passage des `add_time_ids` (incluant taille originale et coordonn√©es de recadrage) au mod√®le UNet, conform√©ment aux exigences de SDXL.
        *   L'encodage VAE est effectu√© en dehors du contexte `autocast` si le VAE est en fp32 et l'entra√Ænement en fp16/bf16.
        *   Ajout du *gradient clipping* pendant l'entra√Ænement pour stabiliser la convergence.
        *   Sauvegarde finale du LoRA en un unique fichier `.safetensors` contenant les poids de l'UNet et optionnellement des encodeurs de texte.
        *   Utilisation de `unet.add_adapter()` et `text_encoder.add_adapter()` pour une configuration PEFT plus moderne.
        *   Utilisation de `cast_training_params()` pour convertir les param√®tres LoRA en fp32 lors de l'entra√Ænement en fp16.
    *   **Interface Utilisateur (UI)**:
        *   Le taux d'apprentissage (learning rate) est maintenant s√©lectionnable via un menu d√©roulant avec des descriptions pour chaque valeur.
        *   La s√©lection du mod√®le de base est un menu d√©roulant.
        *   Les options d'optimiseur, de planificateur de taux d'apprentissage et de pr√©cision mixte sont des menus d√©roulants.
        *   Les options avanc√©es de r√©seau et d'optimiseur sont group√©es dans des accord√©ons.

*   **Gestion de la M√©moire (`Utils/gest_mem.py`)**:
    *   Nouveau module utilitaire pour surveiller l'utilisation des ressources syst√®me (RAM, CPU, VRAM, Utilisation GPU).
    *   Utilise `psutil` pour les statistiques RAM et CPU.
    *   Utilise `pynvml` (si disponible) pour des statistiques VRAM d√©taill√©es et l'utilisation GPU pour les cartes NVIDIA.
    *   Fallback sur `torch.cuda` pour les informations VRAM de base si `pynvml` n'est pas disponible.
    *   Affiche les statistiques via des barres de progression circulaires SVG dans l'interface utilisateur.
    *   Fournit un accord√©on "Gestion de la M√©moire" dans l'interface utilisateur avec :
        *   Affichage en direct (optionnel) des statistiques.
        *   Un bouton "D√©charger Tous les Mod√®les" qui interagit avec `ModelManager`.
        *   Nettoyage explicite de la m√©moire (`gc.collect()`, `torch.cuda.empty_cache()`) apr√®s le d√©chargement.

*   **CogView3-Plus (`CogView3Plus_mod.py`)**:
    *   Logique de d√©termination des r√©solutions autoris√©es am√©lior√©e, avec fallback sur la configuration `FORMATS` si `COGVIEW3PLUS_ALLOWED_RESOLUTIONS` n'est pas d√©finie.
    *   Le chargement du mod√®le indique maintenant que les configurations (offload, slicing, tiling) sont g√©r√©es par le `ModelManager`.
    *   Utilisation de `execute_pipeline_task_async` pour la g√©n√©ration d'images, permettant une interface utilisateur plus r√©active.
    *   Nettoyage explicite de la m√©moire (`del`, `gc.collect()`, `torch.cuda.empty_cache()`) apr√®s la g√©n√©ration de chaque image dans un lot.
    *   Les m√©tadonn√©es des images sauvegard√©es incluent `Module: "CogView3-Plus"` et `Model: THUDM/CogView3-Plus-3B`.

*   **CogView4 (`CogView4_mod.py`)**:
    *   Logique de d√©termination des r√©solutions autoris√©es similaire √† CogView3Plus, utilisant `COGVIEW4_ALLOWED_RESOLUTIONS`.
    *   Le chargement du mod√®le applique les configurations sp√©cifiques (CPU offload, VAE slicing/tiling) *apr√®s* le chargement du pipeline `CogView4Pipeline`.
    *   Utilisation de `execute_pipeline_task_async` pour la g√©n√©ration.
    *   Les m√©tadonn√©es des images sauvegard√©es incluent `Module: "CogView4"` et `Model: THUDM/CogView4-6B`.

### üêõ Corrections de Bugs üêõ

*   **LoRATraining (`LoRATraining_mod.py`)**:
    *   Correction d'un bug potentiel o√π `is_preparing` n'√©tait pas correctement utilis√©, remplac√© par `self.is_preparing`.
    *   Assure que `is_preparing` est mis √† `False` lorsque l'entra√Ænement d√©marre.

### ‚öôÔ∏è Technique et Refactoring ‚öôÔ∏è

*   **G√©n√©ral**:
    *   Les modules utilisent maintenant `self.module_translations` qui est initialis√© avec les traductions fusionn√©es (globales + sp√©cifiques au module) lors de l'initialisation du module par `GestionModule`.
*   **ModelManager (`Utils/model_manager.py`)**:
    *   La m√©thode `load_model` g√®re maintenant les types de mod√®les `sana_sprint`, `cogview4`, `cogview3plus` et applique des configurations sp√©cifiques (dtype, offload) pour ces mod√®les.
    *   La m√©thode `unload_model` a √©t√© am√©lior√©e pour une suppression plus explicite des composants du pipeline.
    *   La m√©thode `apply_loras` a √©t√© revue pour utiliser `unload_lora_weights` et `set_adapters` de mani√®re plus robuste.
*   **LLM Prompter (`Utils/llm_prompter_util.py`)**:
    *   Utilise `AutoModelForCausalLM` et `AutoTokenizer` pour une compatibilit√© plus large avec les mod√®les Hugging Face.
    *   Le mod√®le est charg√© sur CPU (`device_map="cpu"`) pour √©viter les conflits de VRAM.
    *   Le `pad_token` du tokenizer est d√©fini sur `eos_token` si non pr√©sent, ce qui est n√©cessaire pour certains mod√®les comme Qwen.
    *   Am√©lioration du parsing de la sortie du LLM pour extraire le prompt, en g√©rant les balises `<think>` et les pr√©ambules courants.

---


## Beta 1.9.0 üêîThe Chicken Arrivesüêî

*Date: 2025-05-29*

### ‚ú® New Features / Nouvelles Fonctionnalit√©s

*   **New Module: Image ReLighting (`reLighting_mod.py`)**
    *   Introduced a new tab for advanced image relighting using IC-Light models. This module is based on the excellent work by lllyasviel/IC-Light.
    *   Supports two main modes:
        *   **FC (Foreground Conditioned):** Relights a subject based on the foreground image and a chosen light direction (e.g., left, right, top, bottom, or none).
        *   **FBC (Foreground-Background Conditioned):** Relights a subject considering both a foreground image and a background. The background can be uploaded, flipped, or generated as a directional light source or ambient grey.
    *   Integrates automatic background removal for the foreground subject using BriaRMBG.
    *   Offers comprehensive controls: prompt, negative prompt, seed, steps, CFG scale, high-resolution upscaling with denoising, and mode-specific parameters.
    *   Saves relighted images with detailed generation metadata.
*   **Nouveau Module : Re-√âclairage d'Image (`reLighting_mod.py`)**
    *   Introduction d'un nouvel onglet pour le re-√©clairage avanc√© d'images utilisant les mod√®les IC-Light. Ce module est bas√© sur l'excellent travail de lllyasviel/IC-Light.
    *   Supporte deux modes principaux :
        *   **FC (Conditionn√© par l'Avant-plan) :** R√©-√©claire un sujet en se basant sur l'image d'avant-plan et une direction de lumi√®re choisie (ex: gauche, droite, haut, bas, ou aucune).
        *   **FBC (Conditionn√© par l'Avant-plan et l'Arri√®re-plan) :** R√©-√©claire un sujet en consid√©rant √† la fois une image d'avant-plan et un arri√®re-plan. L'arri√®re-plan peut √™tre t√©l√©vers√©, invers√©, ou g√©n√©r√© comme une source de lumi√®re directionnelle ou un gris ambiant.
    *   Int√®gre la suppression automatique de l'arri√®re-plan pour le sujet d'avant-plan en utilisant BriaRMBG.
    *   Offre des contr√¥les complets : prompt, prompt n√©gatif, seed, √©tapes, √©chelle CFG, mise √† l'√©chelle haute r√©solution avec d√©bruitage, et param√®tres sp√©cifiques au mode.
    *   Sauvegarde les images r√©-√©clair√©es avec des m√©tadonn√©es de g√©n√©ration d√©taill√©es.

---

## Beta 1.8.9 üê£The Chick, Future Chickenüêî

### ‚ú® New Features / Nouvelles Fonctionnalit√©s

*   **AI Prompt Enhancement (LLM):** Added an optional feature to automatically enrich user prompts using a local Language Model (default: `Qwen/Qwen3-0.6B`). The LLM generates more detailed and imaginative prompts in English, optimized for image generators. This feature is configurable via the `LLM_PROMPTER_MODEL_PATH` key in `config.json` and runs on the CPU to preserve GPU resources.
    *   **Am√©lioration des Prompts par IA (LLM) :** Ajout d'une fonctionnalit√© optionnelle pour enrichir automatiquement les prompts utilisateurs en utilisant un Mod√®le de Langage local (par d√©faut : `Qwen/Qwen3-0.6B`). Le LLM g√©n√®re des prompts plus d√©taill√©s et imaginatifs en anglais, optimis√©s pour les g√©n√©rateurs d'images. Cette fonctionnalit√© est configurable via la cl√© `LLM_PROMPTER_MODEL_PATH` dans `config.json` et s'ex√©cute sur le CPU pour pr√©server les ressources GPU.

### üõ†Ô∏è Fixes / Corrections

*   **Module Translation:** Fixed a major bug where the selected language in `config.json` (e.g., English) was not correctly passed during module initialization, leading to UI translation issues within modules. `GestionModule` now correctly receives and applies the global language and translations.
    *   **Traduction des Modules :** Correction d'un bug majeur o√π la langue s√©lectionn√©e dans `config.json` (par exemple, l'anglais) n'√©tait pas correctement transmise lors de l'initialisation des modules, entra√Ænant des probl√®mes de traduction de l'interface des modules. `GestionModule` re√ßoit et applique maintenant correctement la langue et les traductions globales.

---

## Beta 1.8.8 üê•Crazy Happy Chicküê•

*Date: 2025-05-14*

### üîß Changes

*   **UI/UX - LoRA Loading:** LoRA dropdown menus are now populated with available LoRAs upon application startup, improving initial usability by removing the need to manually refresh the list.
*   **Gradio Update:** The application has been updated to be compatible with Gradio `5.29.1`.


### üõ†Ô∏è Fixes

*   **Preset Loading - VAE:** Corrected an issue where the VAE specified in a loaded preset was not properly selected in the VAE dropdown menu on the image generation interface.

---

### üîß Changements

*   **UI/UX - Chargement des LoRAs :** Les menus d√©roulants des LoRAs sont d√©sormais remplis avec les LoRAs disponibles d√®s le d√©marrage de l'application, am√©liorant l'utilisabilit√© initiale en supprimant le besoin de rafra√Æchir manuellement la liste.
*   **Mise √† Jour Gradio :** L'application a √©t√© mise √† jour pour √™tre compatible avec Gradio `5.29.1`.


### üõ†Ô∏è Corrections

*   **Chargement des Presets - VAE :** Correction d'un probl√®me o√π le VAE sp√©cifi√© dans un preset charg√© n'√©tait pas correctement s√©lectionn√© dans le menu d√©roulant VAE de l'interface de g√©n√©ration d'images.

---

## Beta 1.8.7 üê•Crazy Happy Chicküê•

*Date: 2025-05-13*

### ‚ú® New Features

*   **New Module: Civitai Downloader (`civitai_downloader_mod.py`)**
    *   Added a dedicated tab to search and download models, LoRAs, VAEs, etc., directly from Civitai.
    *   Supports filtering by model type, sort order, period, and NSFW content.
    *   Includes an interface to view model details, select specific versions and files for download.
    *   Option to use a Civitai API key for extended access.
*   **New Module: Image Watermark (`ImageWatermark_mod.py`)**
    *   Added a new tab for applying text or image watermarks to your generated images.
    *   Supports single image processing and batch processing of images from a folder.
    *   Customizable options for watermark content (text/image), font, size, color, scale, opacity, position (including tiling), margin, and rotation.

### üîß Changes

*   **Gradio Update:** The application has been updated to be compatible with Gradio `5.29.0`. 

### üõ†Ô∏è Fixes

*   **HTML Report Generation:** Improved HTML report generation to ensure it's correctly created or updated even if the image generation process is stopped prematurely.
*   **General Bug Fixes:** Addressed various minor bugs and improved overall stability.

---

### ‚ú® Nouvelles Fonctionnalit√©s (French)

*   **Nouveau Module : T√©l√©chargeur Civitai (`civitai_downloader_mod.py`)**
    *   Ajout d'un onglet d√©di√© pour rechercher et t√©l√©charger des mod√®les, LoRAs, VAEs, etc., directement depuis Civitai.
    *   Supporte le filtrage par type de mod√®le, ordre de tri, p√©riode et contenu NSFW.
    *   Inclut une interface pour voir les d√©tails du mod√®le, s√©lectionner des versions sp√©cifiques et des fichiers √† t√©l√©charger.
    *   Option d'utiliser une cl√© API Civitai pour un acc√®s √©tendu.
*   **Nouveau Module : Filigrane d'Image (`ImageWatermark_mod.py`)**
    *   Ajout d'un nouvel onglet pour appliquer des filigranes textuels ou graphiques sur vos images g√©n√©r√©es.
    *   Supporte le traitement d'image unique et le traitement par lot d'images depuis un dossier.
    *   Options personnalisables pour le contenu du filigrane (texte/image), police, taille, couleur, √©chelle, opacit√©, position (y compris en mosa√Øque), marge et rotation.

### üîß Changements (French)

*   **Mise √† Jour Gradio :** L'application a √©t√© mise √† jour pour √™tre compatible avec Gradio `5.29.0`. 

### üõ†Ô∏è Corrections (French)

*   **G√©n√©ration du Rapport HTML :** Am√©lioration de la g√©n√©ration du rapport HTML pour s'assurer qu'il est correctement cr√©√© ou mis √† jour m√™me si le processus de g√©n√©ration d'images est arr√™t√© pr√©matur√©ment.
*   **Corrections de Bugs G√©n√©rales :** R√©solution de divers bugs mineurs et am√©lioration de la stabilit√© g√©n√©rale.

---

## Beta 1.8.6 üê•Crazy Happy Chicküê•

*Date: 2025-05-02*

### ‚ú® New Features

*   **New Module: Sana Sprint (`sana_sprint_mod.py`)**
    *   Added a dedicated tab for image generation using the `Efficient-Large-Model/Sana_Sprint_0.6B_1024px_diffusers` model.
    *   Supports text-to-image generation with style mixing.
    *   Includes image-to-prompt functionality: generate a prompt directly from an uploaded image within the Sana Sprint tab.
    *   Optimized for speed with fixed steps (2) and output size (1024x1024).
*   **Image-to-Prompt Refactoring:**
    *   Isolated the image-to-prompt generation logic (using `MiaoshouAI/Florence-2-base-PromptGen-v2.0`) into a reusable module: `core/image_prompter.py`.
    *   This functionality is now used by both the main generation tab and the Sana Sprint module.
    *   Model loading is handled centrally and initialized at application startup.

### üõ†Ô∏è Fixes

*   **Gradio Dropdown Warnings:** Resolved persistent `UserWarning: The value passed into gr.Dropdown() is not in the list of choices...` by:
    *   Adding `allow_custom_value=True` to relevant dropdown components (`model`, `VAE`, `sampler`, `format`, `LoRA`, `preset filters`, etc.) across the application.
    *   Improving the logic in `ModelManager.list_models` to filter out the placeholder `your_default_modele.safetensors` from the choices list.
*   **Module Stop Button:** Corrected `UserWarning` related to argument mismatch when calling the `stop_generation` method in modules like `image_to_image_mod.py` by passing necessary arguments (like translations) via `gr.State`.

---

### ‚ú® Nouvelles Fonctionnalit√©s (French)

*   **Nouveau Module : Sana Sprint (`sana_sprint_mod.py`)**
    *   Ajout d'un onglet d√©di√© pour la g√©n√©ration d'images avec le mod√®le `Efficient-Large-Model/Sana_Sprint_0.6B_1024px_diffusers`.
    *   Supporte la g√©n√©ration texte-vers-image avec m√©lange de styles.
    *   Inclut la fonctionnalit√© image-vers-prompt : g√©n√©rer un prompt directement depuis une image t√©l√©vers√©e dans l'onglet Sana Sprint.
    *   Optimis√© pour la vitesse avec des √©tapes fixes (2) et une taille de sortie fixe (1024x1024).
*   **Refactorisation Image-vers-Prompt :**
    *   Isolation de la logique de g√©n√©ration de prompt depuis une image (utilisant `MiaoshouAI/Florence-2-base-PromptGen-v2.0`) dans un module r√©utilisable : `core/image_prompter.py`.
    *   Cette fonctionnalit√© est maintenant utilis√©e par l'onglet de g√©n√©ration principal et le module Sana Sprint.
    *   Le chargement du mod√®le est g√©r√© de mani√®re centralis√©e et initialis√© au d√©marrage de l'application.

### üõ†Ô∏è Corrections (French)

*   **Avertissements Dropdown Gradio :** R√©solution des avertissements persistants `UserWarning: The value passed into gr.Dropdown() is not in the list of choices...` en :
    *   Ajoutant `allow_custom_value=True` aux composants dropdown concern√©s (`mod√®le`, `VAE`, `sampler`, `format`, `LoRA`, `filtres presets`, etc.) dans toute l'application.
    *   Am√©liorant la logique dans `ModelManager.list_models` pour filtrer la valeur placeholder `your_default_modele.safetensors` de la liste des choix.
*   **Bouton Stop des Modules :** Correction du `UserWarning` li√© √† une incoh√©rence d'arguments lors de l'appel de la m√©thode `stop_generation` dans les modules (ex: `image_to_image_mod.py`) en passant les arguments n√©cessaires (comme les traductions) via `gr.State`.

---

## Beta 1.8.5 üê•Crazy Happy Chicküê•

*Date: 2025-05-02*

### ‚ú® New Features

*   **New Module: Image Enhancement (`ImageEnhancement_mod.py`)**
    *   Added a dedicated tab replacing previous Upscaling (SDXL) and Enhancement (AuraSR) functionalities.
    *   **Colorization:** Integrated ModelScope's `damo/cv_ddcolor_image-colorization` model to colorize black and white images.
    *   **Upscale (4x):** Integrated Diffusers' `CompVis/ldm-super-resolution-4x-openimages` model for 4x image upscaling.
    *   **Restoration:** Integrated the OneRestore model (`onerestore_real.tar` + `embedder_model.tar`) for automatic image degradation detection and restoration (e.g., fixing blur, noise). Models are included in `modules/ImageEnhancement_models/`.
    *   **Auto Retouch:** Added a simple automatic retouching option using PIL enhancements (Contrast, Sharpness, Saturation).
*   **Model Management:** Enhancement models are loaded on demand and unloaded after use to save VRAM, including automatic unloading of the main generation model if loaded.
*   **Helper Functions:** Created `ImageEnhancement_helper.py` for loading OneRestore checkpoints.

### üîß Changes

*   **Auto Retouch Enhancement:** Added saturation adjustment to the existing "Auto Retouch" feature within the Image Enhancement module.
*   **Dependencies:** Updated and pinned requirements in `requirements.txt` for better stability and reproducibility. Switched `diffusers` from a Git commit to the stable PyPI version (`0.33.1`) to support future features (like SANA). Corrected `opencv-python-headless` version. Removed unnecessary `futures` package.
*   **Module Cleanup:** Removed obsolete AuraSR enhancement and SDXL Upscaling modules/features, now superseded by the new Image Enhancement module.
*   **Code Quality:** Minor internal code adjustments and cleanup in various modules.

### ‚ûï Added Features

*   **XMP Metadata:** Images are now saved with XMP metadata, enriching PNG, JPEG, and WEBP files with comprehensive information about their generation. For example:
    *   **PNG:** Metadata is stored using `pnginfo`.
    *   **JPEG:** Metadata is stored in `exif.UserComment`.
    *   **WEBP:** Metadata is stored in `xmp` format.
* **Image to Image Batch Mode:** Image to Image module now allows you to select a folder containing multiple images for processing in batch mode.

---

### ‚ú® Nouvelles Fonctionnalit√©s (French)

*   **Nouveau Module : Am√©lioration d'Image (`ImageEnhancement_mod.py`)**
    *   Ajout d'un onglet d√©di√© rempla√ßant les fonctionnalit√©s pr√©c√©dentes d'Upscaling (SDXL) et d'Am√©lioration (AuraSR).
    *   **Colorisation :** Int√©gration du mod√®le ModelScope `damo/cv_ddcolor_image-colorization` pour coloriser les images en noir et blanc.
    *   **Upscale (4x) :** Int√©gration du mod√®le Diffusers `CompVis/ldm-super-resolution-4x-openimages` pour l'agrandissement d'image 4x.
    *   **Restauration :** Int√©gration du mod√®le OneRestore (`onerestore_real.tar` + `embedder_model.tar`) pour la d√©tection automatique de la d√©gradation et la restauration d'image (ex: correction du flou, bruit). Mod√®les inclus dans `modules/ImageEnhancement_models/`.
    *   **Retouche Auto :** Ajout d'une option de retouche automatique simple utilisant les am√©liorations PIL (Contraste, Nettet√©, Saturation).
*   **Gestion des Mod√®les :** Les mod√®les d'am√©lioration sont charg√©s √† la demande et d√©charg√©s apr√®s utilisation pour √©conomiser la VRAM, incluant le d√©chargement automatique du mod√®le de g√©n√©ration principal s'il est charg√©.
*   **Fonctions Utilitaires :** Cr√©ation de `ImageEnhancement_helper.py` pour le chargement des checkpoints OneRestore.

### üîß Changements (French)

*   **Am√©lioration Retouche Auto :** Ajout de l'ajustement de la saturation √† la fonctionnalit√© "Retouche Auto" existante dans le module d'Am√©lioration d'Image.
*   **D√©pendances :** Mise √† jour et √©pinglage des d√©pendances dans `requirements.txt` pour une meilleure stabilit√© et reproductibilit√©. Remplacement de `diffusers` d'un commit Git vers la version stable PyPI (`0.33.1`) pour supporter les fonctionnalit√©s futures (comme SANA). Correction de la version de `opencv-python-headless`. Suppression du paquet `futures` inutile.
*   **Nettoyage Modules :** Suppression des modules/fonctionnalit√©s obsol√®tes d'am√©lioration AuraSR et d'Upscaling SDXL, d√©sormais remplac√©s par le nouveau module d'Am√©lioration d'Image.
*   **Qualit√© du Code :** Ajustements mineurs du code interne et nettoyage dans divers modules.

### ‚ûï Fonctionnalit√©s Ajout√©es

*   **M√©tadonn√©es XMP :** Les images sont d√©sormais sauvegard√©es avec des m√©tadonn√©es XMP, enrichissant les fichiers PNG, JPEG et WEBP avec des informations compl√®tes sur leur g√©n√©ration. Par exemple :
    *   **PNG :** Les m√©tadonn√©es sont stock√©es en utilisant `pnginfo`.
    *   **JPEG :** Les m√©tadonn√©es sont stock√©es dans `exif.UserComment`.
    *   **WEBP :** Les m√©tadonn√©es sont stock√©es au format `xmp`.
*   **Mode Batch Image to Image :** Le module Image to Image vous permet maintenant de s√©lectionner un dossier contenant plusieurs images pour un traitement en mode batch.
---


## Beta 1.8 üöÄ

*Date: 2025-04-29*

### üåü Summary of Version [Beta 1.8]

#### ‚ú® New Features

*   **Batch Generator Tab:**
    *   Added a new dedicated tab to easily create lists of image generation tasks (batches).
    *   Configure model, VAE, prompts, negative prompts, styles, sampler, steps, guidance, seed, dimensions, LoRAs (up to 4), and optional output filename for each task.
    *   Includes an option to automatically translate the positive prompt to English before adding it to the task.
    *   Displays the list of tasks in a table for review.
    *   Generates a JSON file containing the batch definition.
    *   **Automatic Saving:** The generated JSON is automatically saved to a predefined directory (`Output\json_batch_files` by default, configurable in `config.json` via `SAVE_BATCH_JSON_PATH`) with an incremental filename (e.g., `batch_001.json`, `batch_002.json`).
*   **Batch Runner Integration:**
    *   Added functionality within the main generation tab (under an accordion) to load and execute batch tasks defined in a JSON file.
    *   Loads the JSON file (ideally from the configured save path, e.g., using `gr.FileExplorer` if applicable).
    *   Processes tasks sequentially, automatically handling model/VAE/sampler loading and unloading to optimize performance.
    *   Applies LoRAs specified for each task.
    *   Displays overall batch progress, individual task progress, and generated images in a gallery.
    *   Includes a button to stop the entire batch process.
    *   Saves generated images and creates the HTML report similar to single image generation.

#### üîß Changes

*   **Configuration:** Added `SAVE_BATCH_JSON_PATH` to `config.json` to define the default save location for batch JSON files.
*   **UI:** Integrated batch generation creation into its own tab and batch execution controls into the main generation tab.
*   **Core Logic:** Implemented `batch_runner.py` to handle the execution logic for batch files, including model management and task processing. Standardized module JSON structure for translations (`language` key).
*   **Pipeline Execution Refactoring:** Refactored the core pipeline execution logic into `pipeline_executor.py` for better separation of concerns and asynchronous handling, improving UI responsiveness during generation.
*   **Dependency Update:** Tested and confirmed compatibility with **PyTorch 2.7** and **CUDA 12.8**, potentially offering performance improvements. The installation script (`install.bat`) has been updated accordingly.

#### üõ†Ô∏è Fixes

*   **Preset Loading:** Corrected an issue where loading a preset with the default VAE ("D√©faut VAE") would incorrectly display a "VAE not found" warning.
*   **Translation Loading:** Fixed issues related to loading translations for module-specific UI elements by standardizing the JSON structure (`language` key) and correcting translation function calls (`.format()` usage).
*   **Callback Errors:** Resolved `'NoneType' object has no attribute 'append'` errors in the progress callback when running in batch mode.
*   **JSON Parsing:** Corrected `TypeError` when handling `styles` and `loras` data already parsed from JSON in the batch runner.

### üåü R√©sum√© de la Version [Beta 1.8]

#### ‚ú® Nouvelles Fonctionnalit√©s

*   **Onglet G√©n√©rateur de Batch:**
    *   Ajout d'un nouvel onglet d√©di√© pour cr√©er facilement des listes de t√¢ches de g√©n√©ration d'images (batches).
    *   Permet de configurer le mod√®le, VAE, prompts, prompt n√©gatif, styles, sampler, √©tapes, guidage, seed, dimensions, LoRAs (jusqu'√† 4), et un nom de fichier de sortie optionnel pour chaque t√¢che.
    *   Inclut une option pour traduire automatiquement le prompt positif en anglais avant de l'ajouter √† la t√¢che.
    *   Affiche la liste des t√¢ches dans un tableau pour v√©rification.
    *   G√©n√®re un fichier JSON contenant la d√©finition du batch.
    *   **Sauvegarde Automatique:** Le JSON g√©n√©r√© est automatiquement sauvegard√© dans un r√©pertoire pr√©d√©fini (`Output\json_batch_files` par d√©faut, configurable dans `config.json` via `SAVE_BATCH_JSON_PATH`) avec un nom de fichier incr√©mentiel (ex: `batch_001.json`, `batch_002.json`).
*   **Int√©gration de l'Ex√©cuteur de Batch:**
    *   Ajout d'une fonctionnalit√© dans l'onglet de g√©n√©ration principal (sous un accord√©on) pour charger et ex√©cuter les t√¢ches de batch d√©finies dans un fichier JSON.
    *   Charge le fichier JSON (id√©alement depuis le chemin de sauvegarde configur√©).
    *   Traite les t√¢ches s√©quentiellement, g√©rant automatiquement le chargement/d√©chargement des mod√®les/VAE/samplers pour optimiser les performances.
    *   Applique les LoRAs sp√©cifi√©s pour chaque t√¢che.
    *   Affiche la progression globale du batch, la progression de la t√¢che individuelle, et les images g√©n√©r√©es dans une galerie.
    *   Inclut un bouton pour arr√™ter l'ensemble du processus de batch.
    *   Sauvegarde les images g√©n√©r√©es et cr√©e le rapport HTML de mani√®re similaire √† la g√©n√©ration d'image unique.

#### üîß Changements

*   **Configuration:** Ajout de `SAVE_BATCH_JSON_PATH` dans `config.json` pour d√©finir l'emplacement de sauvegarde par d√©faut des fichiers JSON de batch.
*   **UI:** Int√©gration de la cr√©ation de batch dans son propre onglet et des contr√¥les d'ex√©cution de batch dans l'onglet de g√©n√©ration principal.
*   **Logique Principale:** Impl√©mentation de `batch_runner.py` pour g√©rer la logique d'ex√©cution des fichiers batch, y compris la gestion des mod√®les et le traitement des t√¢ches. Standardisation de la structure JSON des modules pour les traductions (cl√© `language`).
*   **Refactorisation Ex√©cution Pipeline:** Refactorisation de la logique principale d'ex√©cution du pipeline dans `pipeline_executor.py` pour une meilleure s√©paration des responsabilit√©s et une gestion asynchrone, am√©liorant la r√©activit√© de l'interface pendant la g√©n√©ration.
*   **Mise √† Jour D√©pendances:** Test√© et confirm√© compatible avec **PyTorch 2.7** et **CUDA 12.8**, offrant potentiellement des am√©liorations de performance. Le script d'installation (`install.bat`) a √©t√© mis √† jour en cons√©quence.

#### üõ†Ô∏è Corrections

*   **Chargement des Presets:** Correction d'un probl√®me o√π le chargement d'un preset avec le VAE par d√©faut ("D√©faut VAE") affichait incorrectement un avertissement "VAE introuvable".
*   **Chargement des Traductions:** Correction de probl√®mes li√©s au chargement des traductions pour les √©l√©ments d'interface sp√©cifiques aux modules en standardisant la structure JSON (cl√© `language`) et en corrigeant les appels √† la fonction de traduction (usage de `.format()`).
*   **Erreurs Callback:** R√©solution des erreurs `'NoneType' object has no attribute 'append'` dans le callback de progression lors de l'ex√©cution en mode batch.
*   **Analyse JSON:** Correction du `TypeError` lors de la manipulation des donn√©es `styles` et `loras` d√©j√† analys√©es depuis le JSON dans l'ex√©cuteur de batch.
